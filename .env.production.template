# =============================================================================
# Engram Production Environment Configuration
# =============================================================================
#
# INSTRUCTIONS:
#   1. Copy this file to .env in your deployment directory
#   2. Replace all placeholder values with your actual credentials
#   3. NEVER commit the .env file to version control
#   4. Keep this file secure - it contains sensitive credentials
#
# DEPLOYMENT TARGET: Hetzner Cloud VPS (Ashburn, VA)
# ARCHITECTURE: Docker Compose with Hugging Face Inference API
#
# =============================================================================

# =============================================================================
# DATABASE CREDENTIALS
# =============================================================================

# PostgreSQL - Used by API (keys/auth) and Tuner (Optuna studies)
# WARNING: Change these defaults immediately in production!
POSTGRES_USER=engram
POSTGRES_PASSWORD=change-this-password-to-a-strong-random-value
POSTGRES_DB=engram

# =============================================================================
# HUGGING FACE CONFIGURATION
# =============================================================================

# Hugging Face API Token
# Get your token from: https://huggingface.co/settings/tokens
# Required for: Embeddings and reranking via Inference API
# WARNING: Keep this secret! It has access to your HF account.
HF_API_TOKEN=hf_your_token_here

# ML Backend Configuration
# Set to "huggingface" to use Inference API instead of local models
# This saves CPU/RAM on the VPS by offloading ML to HF servers
EMBEDDER_BACKEND=huggingface
RERANKER_BACKEND=huggingface

# =============================================================================
# SERVICE URLS (Internal Docker Network)
# =============================================================================
# These use Docker Compose service names for container-to-container communication

# Vector Database
QDRANT_URL=http://qdrant:6333
QDRANT_COLLECTION=engram_memory
QDRANT_TURNS_COLLECTION=engram_turns

# Graph Database
FALKORDB_URL=redis://falkordb:6379

# Cache & Pub/Sub
REDIS_URL=redis://falkordb:6379

# Event Streaming
KAFKA_BOOTSTRAP_SERVERS=redpanda:9092

# Search Service (for API)
SEARCH_URL=http://search:5002

# PostgreSQL Connection String (for API)
DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}

# =============================================================================
# SEARCH SERVICE CONFIGURATION
# =============================================================================

# Device for local inference (only used if EMBEDDER_BACKEND=local)
# Options: cpu, cuda, mps
EMBEDDER_DEVICE=cpu

# Model Selection (HuggingFace Serverless Compatible)
EMBEDDER_TEXT_MODEL=BAAI/bge-small-en-v1.5
EMBEDDER_CODE_MODEL=nomic-ai/nomic-embed-text-v1.5

# Reranker Models
RERANKER_FAST_MODEL=ms-marco-TinyBERT-L-2-v2
RERANKER_ACCURATE_MODEL=BAAI/bge-reranker-v2-m3
RERANKER_CODE_MODEL=jinaai/jina-reranker-v2-base-multilingual

# Search Behavior
SEARCH_DEFAULT_LIMIT=10
SEARCH_MAX_LIMIT=100
SEARCH_RERANK_DEPTH=30

# Model Preloading (disable for HuggingFace backend to save memory)
EMBEDDER_PRELOAD=false

# =============================================================================
# API SERVICE CONFIGURATION
# =============================================================================

# Server Settings
PORT=8080
LOG_LEVEL=info

# Rate Limiting (requests per minute per API key)
RATE_LIMIT_RPM=60

# =============================================================================
# TUNER SERVICE CONFIGURATION
# =============================================================================

# Optuna Database (separate schema in PostgreSQL)
# Used for hyperparameter optimization studies
TUNER_DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/optuna

# =============================================================================
# CORS CONFIGURATION
# =============================================================================

# Allowed origins for CORS (JSON array format)
# Add your frontend domain(s) here
CORS_ORIGINS=["http://localhost:3000","http://localhost:5000"]

# =============================================================================
# OPTIONAL: ADVANCED SETTINGS
# =============================================================================

# Kafka Consumer
KAFKA_CONSUMER_ENABLED=true
KAFKA_CONSUMER_GROUP=search-turns-indexer

# Qdrant Advanced
QDRANT_TIMEOUT=30
QDRANT_PREFER_GRPC=false

# Debug Mode (disable in production for security)
DEBUG=false

# =============================================================================
# OPTIONAL: LLM RERANKING (Premium Tier)
# =============================================================================
# Uncomment if you want to use LLM-based reranking (Gemini)

# LLM_RERANK_ENABLED=false
# LLM_RERANK_MODEL=gemini-3-flash-preview
# LLM_RERANK_MAX_CANDIDATES=10
# GOOGLE_GENERATIVE_AI_API_KEY=your-google-api-key-here

# =============================================================================
# SECURITY NOTES
# =============================================================================
#
# 1. POSTGRES_PASSWORD: Use a strong random password (32+ characters)
#    Generate with: openssl rand -base64 32
#
# 2. HF_API_TOKEN: Treat like a password. Don't share or commit it.
#    Rotate tokens periodically from HF settings.
#
# 3. DATABASE_URL: Contains credentials - keep secure!
#
# 4. DEBUG: Always set to false in production to prevent information leakage.
#
# 5. File Permissions: Set .env to 600 (owner read/write only)
#    chmod 600 /opt/engram/.env
#
# =============================================================================
