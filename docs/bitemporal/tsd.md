# Technical Specifications Document (TSD): The Soul - Bitemporal Cognitive Knowledge Graph

**Project Name:** The Soul
**Version:** 2.0.0-CODE
**Status:** DRAFT
**Date:** December 6, 2025
**Reference Documents:** PRD (v2.0.0-CODE), SDD (v2.0.0-CODE)

---

## 1. Introduction

### 1.1 Purpose
This Technical Specifications Document (TSD) defines the detailed technical implementation requirements for "The Soul," a Bitemporal Cognitive Knowledge Graph. It translates the functional requirements of the Product Requirements Document (PRD) and the architectural design of the System Design Document (SDD) into specific technical directives for engineering teams.

### 1.2 Scope
This document covers the backend infrastructure, data ingestion pipelines, graph database schema design, vector indexing strategies, and query processing logic. It specifically addresses the handling of high-volume "thinking tokens," code artifact versioning, and the linkage between agent reasoning and file system mutations.

### 1.3 Definitions
*   **Cognitive Event:** An atomic unit of agent activity, such as a thought, a tool call, or a terminal output.
*   **Thinking Token:** A hidden token generated by an LLM representing internal reasoning, not typically shown to the end-user but critical for debugging.
*   **Bitemporal:** The ability to handle two time dimensions: *Valid Time* (when the fact was true in the real world) and *Transaction Time* (when the fact was recorded in the system).
*   **Cognitive AST:** An Abstract Syntax Tree that maps code structures not just to their syntactic parents, but to the semantic reasoning nodes that generated them.

---

## 2. System Architecture Overview

The system follows a biological metaphor, dividing responsibilities into four distinct subsystems.

### 2.1 The Nervous System (Ingestion & Parsing)
Responsible for the high-throughput intake of raw agent streams. It acts as the initial filter and processor, converting unstructured text streams into structured events.
*   **Core Technology:** Streaming Data Platform (e.g., Redpanda) with WebAssembly (Wasm) transforms.
*   **Role:** Decomposes XML/JSON-RPC streams into atomic events.

### 2.2 The Hippocampus (Episodic Memory)
Responsible for the long-term storage of the "Cognitive AST." It maintains the causal graph of events and the bitemporal state of the file system.
*   **Core Technology:** Graph Database (e.g., FalkorDB).
*   **Role:** Stores Nodes (Thoughts, Files) and Edges (MODIFIES, TRIGGERS).

### 2.3 The Cortex (Semantic Indexing)
Responsible for similarity search and pattern recognition. It allows the system to find related memories based on semantic meaning rather than exact keyword matches.
*   **Core Technology:** Vector Database (e.g., Qdrant).
*   **Role:** Stores high-dimensional embeddings of code and reasoning.

### 2.4 The Motor Cortex (Replay & Execution)
Responsible for reconstructing past states and verifying code execution.
*   **Core Technology:** Deterministic Sandbox (e.g., Wassette).
*   **Role:** Re-hydrates the Virtual File System (VFS) at specific timestamps.

---

## 3. Component Specifications

### 3.1 Ingestion Layer Specifications (The Nervous System)

#### 3.1.1 Stream Protocol Detection
The ingestion component must automatically detect the format of the incoming agent stream.
*   **Requirement:** The parser shall inspect the initial bytes of the payload to distinguish between supported provider formats (e.g., Anthropic XML vs. OpenAI JSON schemas).
*   **Fallback:** If the format is unrecognized, the system shall tag the event as `RAW_UNKNOWN` and store it in a "Dead Letter Queue" for manual inspection.

#### 3.1.2 Event Decomposition Logic
The system must parse interleaved streams where reasoning text, tool calls, and code diffs may be mixed.
*   **Thinking Block Extraction:** The parser shall identify start and end markers for reasoning blocks (e.g., specific XML tags) and extract the enclosed text as a `Thought` event.
*   **Diff Extraction:** The parser shall identify code blocks formatted as diffs or patches. It must parse the diff header to identify the target file path and the line numbers affected.
*   **Tool Call Extraction:** The parser shall validate JSON structures representing function calls, extracting the function name and arguments map.

#### 3.1.3 Data Normalization
All extracted events must be normalized into a standard internal structure before downstream processing.
*   **Standard Fields:** Every event object must contain:
    *   `event_id`: A unique UUID v7 (time-sortable).
    *   `session_id`: The identifier for the agent session.
    *   `timestamp`: UTC timestamp with microsecond precision.
    *   `type`: Enumerated type (THOUGHT, TOOL_CALL, OBSERVATION, DIFF).
    *   `content`: The payload (text, JSON, or diff string).
    *   `metadata`: Key-value pairs for provider-specific attributes (e.g., model name, token count).

### 3.2 Memory Layer Specifications (The Hippocampus)

#### 3.2.1 Graph Ontology
The graph database shall implement the following entity types (Nodes) and relationship types (Edges).

**Nodes:**
*   **Session Node:** Represents the container for the interaction. Attributes: `start_time`, `agent_id`, `user_id`.
*   **UserPrompt Node:** Represents input from the human developer. Attributes: `content`, `timestamp`.
*   **ThoughtNode:** Represents a discrete unit of reasoning. Attributes: `content_summary` (full text stored externally if large), `intent_category` (e.g., Refactor, Debug), `token_count`.
*   **ToolCall Node:** Represents an intent to perform an action. Attributes: `tool_name`, `arguments_hash`.
*   **Observation Node:** Represents the output of a tool. Attributes: `stdout`, `stderr`, `exit_code`, `status`.
*   **CodeArtifact Node:** Represents a specific version of a file or function. Attributes: `filepath`, `language`, `content_hash`, `ast_signature`.
*   **DiffHunk Node:** Represents the specific patch applied. Attributes: `patch_content`, `lines_added`, `lines_removed`.

**Edges:**
*   **MOTIVATED_BY:** Connects a `ThoughtNode` to the `UserPrompt` or previous `Observation` that triggered it.
*   **TRIGGERS:** Connects a `ThoughtNode` to a `ToolCall`.
*   **YIELDS:** Connects a `ToolCall` to an `Observation`.
*   **INTENDED_MODIFICATION:** Connects a `ThoughtNode` to a `CodeArtifact` (representing the *intent* to change a file).
*   **MODIFIES:** Connects a `DiffHunk` to a `CodeArtifact` (representing the *actual* change).
*   **INTRODUCED_ERROR:** Connects a `DiffHunk` to an `Observation` if a subsequent test failure is causally linked to that diff.

#### 3.2.2 Bitemporal State Management
To support time-travel debugging, the graph must support versioning of relationships and properties.
*   **Validity Windows:** All `CodeArtifact` nodes and `MODIFIES` edges must have `valid_from` and `valid_to` timestamps.
*   **Current State Query:** A query for the "current state" of the file system shall filter for edges where `valid_to` is NULL or in the future.
*   **Historical State Query:** A query for the state at time $T$ shall filter for edges where `valid_from` <= $T$ AND (`valid_to` > $T$ OR `valid_to` is NULL).

### 3.3 Search Layer Specifications (The Cortex)

#### 3.3.1 Hybrid Indexing Strategy
The system shall employ a hybrid search strategy combining dense vector retrieval with sparse keyword/graph traversal.

*   **Dense Indexing (Vectors):**
    *   **Reasoning Vectors:** `ThoughtNode` content shall be embedded using a text-optimized model (e.g., for natural language understanding).
    *   **Code Vectors:** `CodeArtifact` and `DiffHunk` content shall be embedded using a code-optimized model.
*   **Sparse Indexing (Graph):**
    *   The system shall maintain an inverted index of file paths, function names, and specific error codes found in `Observation` nodes.

#### 3.3.2 Multi-Vector Retrieval Logic
Search queries shall be processed using a multi-stage pipeline:
1.  **Intent Classification:** The user's natural language query is analyzed to determine if they are looking for *reasoning* ("Why did...") or *code* ("Where is...").
2.  **Vector Search:** The query is embedded and matched against the appropriate vector collection (Reasoning or Code).
3.  **Graph Filtering:** Results are filtered based on graph relationships. For example, if searching for "reasoning behind the change to `auth.ts`", the system searches for `ThoughtNodes` that have a path to `CodeArtifact(filepath="auth.ts")`.

### 3.4 Replay Layer Specifications (The Motor Cortex)

#### 3.4.1 VFS Reconstruction
The system must be able to reconstruct the Virtual File System (VFS) for any given point in the session history.
*   **Snapshotting:** The system shall not store full file copies for every change. It shall store the initial state and a sequence of `DiffHunk`s.
*   **Rehydration:** To view a file at time $T$, the system shall retrieve the base file and sequentially apply all `DiffHunk`s with `valid_from` <= $T$.

---

## 4. Data Management & Storage

### 4.1 Storage Tiering
To manage the high volume of tokens efficiently:
*   **Hot Storage (Graph/Vector DB):** Stores graph topology, embeddings, metadata, and summaries of thoughts.
*   **Cold Storage (Blob Store):** Stores the full raw text of large `ThoughtBlocks` and complete file contents. The Graph nodes shall contain URI references to these blobs.

### 4.2 Data Sanitization
*   **Secret Redaction:** Before any data is persisted to the Graph or Blob storage, it must pass through a sanitization filter. This filter shall use regex patterns and entropy analysis to detect and redact API keys, passwords, and other sensitive credentials, replacing them with a `[REDACTED]` placeholder.

---

## 5. Interface Specifications

### 5.1 Ingestion API
The system shall expose a gRPC or WebSocket endpoint for agents to stream data.
*   **Protocol:** The API shall accept a continuous stream of bytes.
*   **Authentication:** Agents must authenticate via a secure token exchange before opening a stream.

### 5.2 Query API
The system shall expose a GraphQL or REST API for client applications (IDEs, Dashboards).
*   **Semantic Search Endpoint:** Accepts a natural language string and returns a list of `ThoughtNodes` and `CodeArtifacts` with relevance scores.
*   **Lineage Endpoint:** Accepts a file path and line number, returning the chain of `ThoughtNodes` that led to the most recent modification of that line.
*   **Session Replay Endpoint:** Accepts a session ID and timestamp, returning the reconstructed file system state and the sequence of events up to that moment.

---

## 6. Non-Functional Requirements

### 6.1 Scalability
*   **Throughput:** The ingestion layer must support a minimum of 5,000 transactions per second (TPS) to handle concurrent agent sessions.
*   **Concurrency:** The system must support at least 50 active agent sessions simultaneously without degradation in ingestion latency.

### 6.2 Performance
*   **Ingestion Latency:** The time from an agent emitting a token to it being queryable in the graph should be under 2 seconds.
*   **Query Latency:** Semantic search queries should return results in under 200 milliseconds (p95).

### 6.3 Reliability
*   **Data Integrity:** Code diffs and file mutations must be stored with "at-least-once" delivery guarantees. Telemetry logs (stdout/stderr) may be processed with "at-most-once" guarantees if necessary to maintain throughput.

### 6.4 Deployment
*   **Containerization:** All components (Ingestor, Graph DB, Vector DB, API) shall be containerized.
*   **Orchestration:** The system shall be deployable on Kubernetes.
*   **Replication:** The ingestion service shall be deployed as a ReplicaSet to ensure high availability.

---

## 7. Security Specifications

*   **Access Control:** Access to the Query API shall be governed by Role-Based Access Control (RBAC), ensuring developers can only query sessions they are authorized to view.
*   **Encryption:** All data must be encrypted at rest (in the DB and Blob Store) and in transit (TLS 1.3).
*   **Audit Logging:** All queries against the memory system must be logged for audit purposes, tracking who accessed what agent reasoning data.
